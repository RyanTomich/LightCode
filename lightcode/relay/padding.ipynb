{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# single sequence padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_input_padding(prompt, tokenizer):\n",
    "    return tokenizer(prompt, padding='max_length', truncation=True, max_length=10, return_tensors='pt')\n",
    "    # return tokenizer(prompt, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    # return tokenizer(prompt, padding=True, return_tensors='pt')\n",
    "    # return tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "model.eval()\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "prompt = \"My favorite music is  \"\n",
    "# prompt = \"In a galaxy far, far away \"\n",
    "\n",
    "inputs = tokenize_input_padding(prompt, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([464])\n",
      "torch.Size([1, 12, 10, 64])\n",
      "tensor(-1.0961)\n",
      "tensor(-2.2123)\n",
      "tensor(-2.2993)\n",
      "tensor(-1.9828)\n",
      "tensor(-2.2604)\n",
      "tensor(-2.2253)\n",
      "tensor(-1.2784)\n",
      "tensor(-1.2625)\n",
      "tensor(-1.3025)\n",
      "tensor(-1.3278)\n"
     ]
    }
   ],
   "source": [
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "logits = outputs[0]\n",
    "kv_cache = outputs[1]\n",
    "\n",
    "last_token_id = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "print(last_token_id)\n",
    "print(kv_cache[0][0].shape)\n",
    "\n",
    "for i in kv_cache[0][0][0][0]:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 11, 64])\n",
      "tensor(-1.0961)\n",
      "tensor(-2.2123)\n",
      "tensor(-2.2993)\n",
      "tensor(-1.9828)\n",
      "tensor(-2.2604)\n",
      "tensor(-2.2253)\n",
      "tensor(-1.2784)\n",
      "tensor(-1.2625)\n",
      "tensor(-1.3025)\n",
      "tensor(-1.3278)\n",
      "tensor(-1.7075)\n",
      "My favorite music is  The New\n"
     ]
    }
   ],
   "source": [
    "generated = [last_token_id.item()]\n",
    "for i in range(1):\n",
    "    last_token_id = torch.tensor([[last_token_id]], device=device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(last_token_id, past_key_values=kv_cache)\n",
    "\n",
    "    logits = outputs[0]\n",
    "    kv_cache = outputs[1]\n",
    "    print(kv_cache[0][0].shape)\n",
    "\n",
    "\n",
    "    last_token_id = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "    generated.append(last_token_id.item())\n",
    "\n",
    "for i in kv_cache[0][0][0][0]:\n",
    "    print(i[0])\n",
    "\n",
    "generated_text = tokenizer.decode(generated, skip_special_tokens=True)\n",
    "print(f'{prompt}{generated_text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Sequence Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[  464,  2068,  7586, 21831, 50256, 50256],\n",
      "        [   73,  8142,   625,   262, 16931,  3290],\n",
      "        [ 1870,   788,  4539,  1497, 50256, 50256]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# List of sequences\n",
    "sequences = [\n",
    "    \"The quick brown fox\",\n",
    "    \"jumps over the lazy dog\",\n",
    "    \"And then runs away\"\n",
    "]\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "inputs = tokenizer(sequences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "generated_outputs = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    max_length=inputs['input_ids'].shape[1] + 20,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "print(\"Input IDs:\", inputs['input_ids'])\n",
    "print(\"Attention Mask:\", inputs['attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: The quick brown fox\n",
      "Generated Text: The quick brown foxThe quick brown foxThe quick brown foxThe quick brown foxThe quick brown foxThe quick brown fox\n",
      "\n",
      "Input Sequence: jumps over the lazy dog\n",
      "Generated Text: jumps over the lazy dog.\n",
      "\n",
      "\"I'm not going to be able to do that,\" he said. \"I\n",
      "\n",
      "Input Sequence: And then runs away\n",
      "Generated Text: And then runs awayThe next day, he's back in the hospital, and he's still in the hospital. He\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_token_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "generated_texts = [tokenizer.decode(output_ids, skip_special_tokens=True) for output_ids in generated_outputs]\n",
    "\n",
    "for i, text in enumerate(generated_texts):\n",
    "    print(f\"Input Sequence: {sequences[i]}\")\n",
    "    print(f\"Generated Text: {text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
