{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Cache\n",
    "\n",
    "\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# single sequence padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids      : tensor([[    2,     2,     2,     2,     1,  1619, 25448,  4696,   338, 29871]])\n",
      "attention_mask : tensor([[0, 0, 0, 0, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "def tokenize_input_padding(prompt, tokenizer):\n",
    "    return tokenizer(prompt, padding='max_length', truncation=True, max_length=10, return_tensors='pt')\n",
    "    # return tokenizer(prompt, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    # return tokenizer(prompt, padding=True, return_tensors='pt')\n",
    "    # return tokenizer(prompt, return_tensors= 'pt')\n",
    "\n",
    "\n",
    "# model_name = 'gpt2'\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "prompt = \"My favorite music is \"\n",
    "# prompt = \"In a galaxy far, far away \"\n",
    "\n",
    "inputs = tokenize_input_padding(prompt, tokenizer)\n",
    "for i in inputs:\n",
    "    print(f'{i:<15}: {inputs[i]}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([29947])\n",
      "torch.Size([1, 32, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "# Prefill\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask,)\n",
    "\n",
    "logits = outputs[0]\n",
    "kv_cache = outputs[1]\n",
    "\n",
    "last_token_id = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "print(last_token_id)\n",
    "# [(kv)(kv)(kv)]\n",
    "print(kv_cache[0][0].shape)\n",
    "\n",
    "# for i in kv_cache[0][0][0][0]:\n",
    "#     print(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 10, 128]) - 29947\n",
      "torch.Size([1, 32, 10, 128]) - 29900\n",
      "torch.Size([1, 32, 10, 128]) - 29879\n",
      "torch.Size([1, 32, 10, 128]) - 4696\n",
      "torch.Size([1, 32, 10, 128]) - 29889\n",
      "tensor([[    2,     2,     2,     2,     1,  1619, 25448,  4696,   338, 29871]]) + [29947, 29900, 29879, 4696, 29889]\n",
      "My favorite music is  80s music.\n"
     ]
    }
   ],
   "source": [
    "# Decoder\n",
    "generated = [last_token_id.item()]\n",
    "print(f'{kv_cache[0][0].shape} - {last_token_id.item()}')\n",
    "for i in range(4):\n",
    "    last_token_id = torch.tensor([[last_token_id]], device=device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(last_token_id, past_key_values=kv_cache)\n",
    "\n",
    "\n",
    "    logits = outputs[0]\n",
    "    kv_cache = outputs[1]\n",
    "\n",
    "    sliced_kv_cache = []\n",
    "    for layer in kv_cache:\n",
    "        k = layer[0]\n",
    "        v = layer[1]\n",
    "        sliced_k = k[:, :, 1:, :]\n",
    "        sliced_v = v[:, :, 1:, :]\n",
    "        sliced_kv_cache.append( (sliced_k, sliced_v) )\n",
    "\n",
    "    kv_cache = tuple(sliced_kv_cache)\n",
    "\n",
    "    last_token_id = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "    print(f'{kv_cache[0][0].shape} - {last_token_id.item()}')\n",
    "\n",
    "    generated.append(last_token_id.item())\n",
    "\n",
    "# for i in kv_cache[0][0][0][0]:\n",
    "#     print(i[1])\n",
    "\n",
    "print(f'{input_ids} + {generated}')\n",
    "generated_text = tokenizer.decode(generated, skip_special_tokens=True)\n",
    "print(f'{prompt} {generated_text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Sequence Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.23it/s]\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[    2,     1,   450,  4996, 17354,  1701, 29916],\n",
      "        [    1,   432, 17204,   975,   278, 17366, 11203],\n",
      "        [    2,     2,     1,  1126,   769,  6057,  3448],\n",
      "        [    2,     1,  1619, 25448,  4696,   338, 29871]])\n",
      "Attention Mask: tensor([[0, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1]])\n",
      "sequence 0:\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "sequence 1:\n",
      "jumps over the lazy dog.\n",
      "The dog is not lazy,\n",
      "but the cat is.\n",
      "The cat is not lazy\n",
      "\n",
      "sequence 2:\n",
      "And then runs away.\n",
      "I'm not sure what to make of this. I'm not sure what to\n",
      "\n",
      "sequence 3:\n",
      "My favorite music is 80s music. I love the 80s. I love the music, the fashion\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With Padding Batched\n",
    "\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/llama-2-7b-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/llama-2-7b-hf\")\n",
    "\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "\n",
    "# List of sequences\n",
    "sequences = [\n",
    "    \"The quick brown fox\",\n",
    "    \"jumps over the lazy dog\",\n",
    "    \"And then runs away\",\n",
    "    \"My favorite music is \",\n",
    "]\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "inputs = tokenizer(sequences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "generated_outputs = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    max_length=inputs['input_ids'].shape[1] + 20,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=False,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    temperature=None,\n",
    "    top_p=None,\n",
    ")\n",
    "\n",
    "print(\"Input IDs:\", inputs['input_ids'])\n",
    "print(\"Attention Mask:\", inputs['attention_mask'])\n",
    "\n",
    "for idx, gen in enumerate(generated_outputs):\n",
    "    print(f'sequence {idx}:')\n",
    "    print (tokenizer.decode(gen, skip_special_tokens=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2\n",
    "**Sequence 1 : \"The quick brown fox 50256, 50256\"**\n",
    "    batched&padded: The quick brown foxThe quick brown foxThe quick brown foxThe quick brown foxThe quick brown foxThe quick brown fox\n",
    "    regular: The quick brown foxes are a great way to get a little bit of a kick out of your dog. The quick\n",
    "\n",
    "**Sequence 2 : \"jumps over the lazy dog\"**\n",
    "    batched&padded: jumps over the lazy dog. \"I'm not going to be able to do that,\" he said. \"I\n",
    "    regular:jumps over the lazy dog. \"I'm not going to be able to do that,\" he said. \"I\n",
    "\n",
    "**Sequence 3 : \"And then runs away 50256 \"**\n",
    "    batched&padded: And then runs awayThe next day, he's back in the hospital, and he's still in the hospital. He\n",
    "    regular: And then runs away. The next day, he was arrested for driving under the influence. He was charged with\n",
    "\n",
    "**Sequence 4 : \"My favorite music is 50256, 50256\"**\n",
    "    batched&padded: My favorite music is The Beatles. I love the Beatles. I love the Beatles. I love the Beatles. I love\n",
    "    regular: My favorite music is  the  \"The Last of Us\" by the Grateful Dead. I love the song, but I\n",
    "\n",
    "    Everythign that was padded changed ...  \n",
    "\n",
    "\n",
    "## Llama-2-7b-hf \n",
    "**Different results, but both are equally valid** \n",
    "MUST PAD FROM THE LEFT FOR LLAMA RESULTS TO BE OKISH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
